{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "from sklearn import metrics\n",
    "np.random.seed(0)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toKaggle(filename,prSoft):\n",
    "    fh = open(filename,'w') \n",
    "    fh.write('ID,Target\\n')\n",
    "    for i,yi in enumerate(prSoft[:,1].ravel()):\n",
    "        fh.write('{},{}\\n'.format(i+1,yi)) # output each prediction\n",
    "    fh.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy class to simplify AUC computation\n",
    "class dummy(ml.classifier): \n",
    "    def set(self,P):\n",
    "        self.Pr = P\n",
    "        self.classes = np.array([0,1]) \n",
    "    def predictSoft(self, X):\n",
    "        return self.Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.72715\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=None) \n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None) \n",
    "\n",
    "# print(X.shape)\n",
    "# X,Y = ml.shuffleData(X,Y)\n",
    "\n",
    "# # select the first 30,000 (shuffled) data points as training data.\n",
    "# Xtr = X[0:50000,:,]   \n",
    "# Ytr = Y[0:50000,]  \n",
    "\n",
    "# # next 30,000 data points as validation data.\n",
    "# Xva = X[50000:100000,:,]\n",
    "# Yva = Y[50000:100000,]\n",
    "\n",
    "Xtr,Xva,Ytr,Yva = ml.splitData(X,Y,0.8)\n",
    "\n",
    "learner = RandomForestClassifier(n_estimators=200, max_depth=19, random_state=0)\n",
    "learner.fit(Xtr, Ytr)\n",
    "\n",
    "print(\"score = \" + str(learner.score(Xva, Yva)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "Xte = np.genfromtxt('data/X_test.txt', delimiter=None)\n",
    "\n",
    "# Create csv format\n",
    "Pv1 = np.zeros((Xva.shape[0],2))\n",
    "Pe1 = np.zeros((Xte.shape[0],2))\n",
    "ens = dummy() # make dummy learner for computing auc values\n",
    "\n",
    "pv1 = learner.predict_proba(Xva)\n",
    "pe1 = learner.predict_proba(Xte)\n",
    "\n",
    "toKaggle('Pv1.csv',Pv1)\n",
    "toKaggle('Pe1.csv',Pe1)\n",
    "ens.set(Pv1);\n",
    "\n",
    "# Create submission\n",
    "Xte = np.genfromtxt('data/X_test.txt', delimiter=None)\n",
    "print(Xte.shape[0])\n",
    "Yte = np.vstack((np.arange(Xte.shape[0]), learner.predict_proba(Xte)[:,1])).T\n",
    "# Output a file with two columns, a row ID and a confidence in class 1:\n",
    "np.savetxt('Y_submit.txt',Yte,'%d, %.2f',header='ID,Prob1',comments='',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
